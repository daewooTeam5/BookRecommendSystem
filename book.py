import requests
import re
from bs4 import BeautifulSoup
from lxml import html
import csv
import time
import random
import pandas as pd

CSV_HEADER = [
    'id', 'title', 'author', 'publisher', 'image', 'price',
    'genre', 'published_at', 'page', 'introduction'
]

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) Gecko/20100101 Firefox/118.0",
]

# ìƒì„¸íŽ˜ì´ì§€ì—ì„œ ì†Œê°œ & ìª½ìˆ˜ ê°€ì ¸ì˜¤ê¸°
def get_extra_details(book_url):
    try:
        headers = {"User-Agent": random.choice(USER_AGENTS),
                   #"Cookie": "CheckSameSite4=IsValidSameSiteSet; AladdinUser=UID=-994978082&SID=zr5bblB%2f5gRzR%2fQnNI%2f17A%3d%3d; _gcl_au=1.1.876579939.1751874263; _fwb=133sGyoBy5ihSERts21dbaF.1751874262917; _ga=GA1.1.814483838.1751874263; _BS_GUUID=zQgyaX65cHCBV9Ls6LgTi3AYdlvNeuRNk7mv4tAg; _fbp=fb.2.1751874273576.174623020985880236; _ga_YF1PFVVFXL=GS2.1.s1751874273$o1$g1$t1751874280$j53$l0$h0; AladdinUS=6evLM5wQvzTD2JSBWHr8MQ%3d%3d&USA=0; _gcl_gs=2.1.k1$i1755661099$u252127833; _gcl_aw=GCL.1755661105.Cj0KCQjwwZDFBhCpARIsAB95qO0YCPUONEIi1w8pYbRvw3C0OvVwuQ0hWmuZ2rD-ZRocsiVxhPnNJBMaApkiEALw_wcB; ala_qs_use=1; _TRK_AUIDA_13987=2cce00e6306d9f131920cc30d308b085:16; _TRK_ASID_13987=d4b486a4f65d467d4662048011e00681; ASP.NET_SessionId=554ly4icdnfjmpuf4bzw5nuj; Supplier=; QsSNSLoginCookies=returnurl=https%3a%2f%2fwww.aladin.co.kr%2fshop%2fwproduct.aspx%3fItemId%3d369848660&snsAppId=1&SecureOpener=1; AladdinLogin=6evLM5wQvzTD2JSBWHr8MQ%3d%3d; AladdinSite=Aladin; AladdinLoginSNS=UID=-994978082&SNSTYPE=4; Aladin.AuthAdult=mKanQyO2sLYOe4z6Xs0MZg==; wcs_bt=s_1c519d64863a:1755712475|b78bdfda7ab45:1755712475; _ga_VKYKBC0ZHH=GS2.1.s1755712276$o21$g1$t1755712475$j16$l0$h0;",

                   }
        response = requests.get(book_url, headers=headers)
        response.raise_for_status()
        tree = html.fromstring(response.content)

        # ì±… ì†Œê°œ (meta description)
        description_list = tree.xpath('//meta[@name="description"]/@content')
        description = description_list[0].strip() if description_list else None
        published_at = tree.xpath('//*[@id="Ere_prod_allwrap"]/div[3]/div[2]/div[1]/div/ul/li[3]/text()')[1]

        # íŽ˜ì´ì§€ ìˆ˜ ("ìª½" ë“¤ì–´ê°„ í…ìŠ¤íŠ¸ ì°¾ê¸°)
        page_list = tree.xpath('//*[@id="Ere_prod_allwrap"]//li/text()')
        page = None
        for item in page_list:
            if "ìª½" in item:
                page = int(item.replace("ìª½", "").strip())
                break

        return description, page,published_at
    except Exception as e:
        print(f"Error crawling {book_url}: {e}")
        return None, None


# ë¦¬ìŠ¤íŠ¸ íŽ˜ì´ì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
def get_books_from_list(list_url,genere):
    headers = {"User-Agent": random.choice(USER_AGENTS)}
    response = requests.get(list_url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    book_items = soup.select('div.ss_book_box')
    books = []

    for item in book_items:
        try:
            # ë§í¬
            link = item.select_one('a.bo3')['href'] if item.select_one('a.bo3') else None
            # ì œëª©
            title = item.select_one('a.bo3').get_text(strip=True) if item.select_one('a.bo3') else None
            # ì €ìž, ì¶œíŒì‚¬, ì¶œíŒì¼
            info_text = item.select_one('div.ss_book_list').get_text(" ", strip=True)
            author, publisher, published_at = None, None, None
            parts = info_text.split(" | ")

            if len(parts) >= 3:
                author, publisher, published_at = parts[0], parts[1], parts[2].replace(".","-")

            author =parts[0].split("(ì§€ì€ì´)")[0].split(" ")[-2]

            # ë‚ ì§œ ë°ì´í„°ë§Œ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ nnnnë…„ nnì›”
            date_pattern = re.compile(r'\d{4}ë…„ \d{1,2}ì›”')
            date_match = date_pattern.search(parts[2])
            published_at = date_match.group()
            published_at = published_at.replace("ë…„", "-").replace("ì›”", "").replace(" ","")
            # ì›”ì´ í•œìžë¦¬ë©´ ì•žì— 0 ì¶”ê°€
            if len(published_at.split("-")[1]) == 1:
                published_at = published_at.replace("-", "-0", 1)

            # ì´ë¯¸ì§€
            imgs = item.select('.flipcover_in > img')

            if len(imgs) > 1:
                image = imgs[1]['src']  # 2ê°œ ì´ìƒì´ë©´ ë‘ ë²ˆì§¸ ì´ë¯¸ì§€
            elif len(imgs) == 1:
                image = imgs[0]['src']  # 1ê°œë©´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€
            else:
                image = None  # ì´ë¯¸ì§€ ì—†ìœ¼ë©´ None
            # ê°€ê²©
            price_tag = item.select_one('td > div:nth-child(1) > ul > li:nth-child(4) > span:nth-child(1)')  # ì›ëž˜ ë¦¬ìŠ¤íŠ¸ íŽ˜ì´ì§€ì— ìžˆìŒ
            price = None
            if price_tag:
                price_text = price_tag.get_text(strip=True).replace(",", "").replace("ì›", "")
                if price_text.isdigit():
                    price = int(price_text)


            books.append({
                'title': title, 'author': author, 'publisher': publisher,
                'image': image, 'price': price,
                'published_at': published_at,
                'genre': genere,
                'page': None, 'introduction': None,
                'link': link
            })
        except Exception as e:
            print(f"ë¦¬ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    return books

def main():
    #                                                                                                                                            | ì—¬ê¸° ì¤‘ê´„í˜¸
    # base_url = "https://www.aladin.co.kr/shop/wbrowse.aspx?BrowseTarget=List&ViewRowsCount=25&ViewType=Detail&PublishMonth=0&SortOrder=2&page={}&Stockstatus=1&PublishDay=84&CID=170&SearchOption="
    base_url = "https://www.aladin.co.kr/shop/wbrowse.aspx?BrowseTarget=List&ViewRowsCount=25&ViewType=Detail&PublishMonth=0&SortOrder=2&page={}&Stockstatus=1&PublishDay=84&CID=1322&SearchOption="
    # base_url = "https://www.aladin.co.kr/shop/wbrowse.aspx?BrowseTarget=List&ViewRowsCount=25&ViewType=Detail&PublishMonth=0&SortOrder=2&page={}&Stockstatus=1&PublishDay=84&CID=170&SearchOption="
    genere = "ì™¸êµ­ì–´"
    print("ðŸš€ ë¦¬ìŠ¤íŠ¸ íŽ˜ì´ì§€ì—ì„œ ê¸°ë³¸ ë„ì„œ ì •ë³´ ìˆ˜ì§‘ ì‹œìž‘...")

    all_books_data = []
    book_id = 1
    # ê²½ì œ 1-20 itzì»´í“¨í„° 1-19 ë§Œí™” 1-12 ì™¸êµ­ì–´ 1-6

    # ðŸ‘‰ 1 ~ 20 íŽ˜ì´ì§€ê¹Œì§€ ë°˜ë³µ
    for page_num in range(1, 6):
        list_url = base_url.format(page_num)
        print(f"ðŸ“– {page_num}íŽ˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

        books = get_books_from_list(list_url,genere)
        print(f"âœ… {page_num}íŽ˜ì´ì§€: {len(books)}ê¶Œ ë°œê²¬")

        for book in books:
            if not book['link']:
                continue

            intro, page, published_at = get_extra_details(book['link'])

            if not intro or not page:
                print(f"âš ï¸ {book['title']} ìƒì„¸ì •ë³´ ë¶€ì¡± â†’ ìŠ¤í‚µ")
                continue

            book['introduction'] = intro
            book['page'] = page
            # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (ì˜ˆ: "2023ë…„ 1ì›”" â†’ "2023-01-01")
            # í•´ë‹¹ ë°ì´í„°ê°€ ë‚ ì§œ í•œê¸€ í˜¹ì€ ì˜ì–´ê°€ í•˜ë‚˜ë¼ë„ ìžˆì„ê²½ìš°
            print("ëŒ€ì¶©ë‚˜ì™€ìžˆëŠ”ê±°",book['published_at'])
            print("ì œëŒ€ë¡œê°€ì ¸ì˜¨ê±°",published_at)
            if published_at and re.search(r'[ê°€-íž£a-zA-Z,]', published_at):
                print(book['published_at'])
                book['published_at'] = book['published_at']+("-"+ str(random.randint(10, 26)))
            else:
                book['published_at'] = published_at


            all_books_data.append(book)

            #time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    # DataFrameìœ¼ë¡œ ë³€í™˜ í›„ ë„ê°’ ìžˆëŠ” í–‰ ì œê±°
    df = pd.DataFrame(all_books_data, columns=CSV_HEADER)
    print(df.head())  # ë¯¸ë¦¬ë³´ê¸°

    output_filename = f'aladin_books_final_{genere.replace("/", "")}.csv'
    df.to_csv(output_filename, index=False, encoding='utf-8-sig')

    print(f"\nðŸŽ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(df)}ê¶Œ ì €ìž¥ -> '{output_filename}'")

if __name__ == '__main__':
    main()
